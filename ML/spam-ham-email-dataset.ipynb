{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7843142,"sourceType":"datasetVersion","datasetId":4598363}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForSeq2SeqLM\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import resample\nimport pandas as pd\nimport numpy as np\nimport re\nfrom torchinfo import summary\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/190k-spam-ham-email-dataset-for-classification/spam_Emails_data.csv\")\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text):\n    if isinstance(text, str):  # Only process if the input is a string\n        text = text.lower()\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n        return text\n    return \"\"  # Return an empty string for non-string values\ndef encode_values(value):\n    if value == 'Spam':\n        return 1  # Spam\n    else:\n        return 0  # Ham\n        \ndf['text'] = df['text'].apply(clean_text)\ndf['label'] = df['label'].apply(encode_values).astype(int)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[['text','label']]\ndf = df.dropna(subset=['label'])\ndf = df.dropna(subset=['text'])\ndf.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_lengths = [len(str(text).split()) for text in df['text']]\nmax_length = max(text_lengths)\nmin_length = min(text_lengths)\naverage_length = np.mean(text_lengths)\nprint(f\"Max length: {max_length}\")\nprint(f\"Min length: {min_length}\")\nprint(f\"Average length: {average_length}\")\nprint()\nplt.figure(figsize=(10, 6))\nplt.hist(text_lengths, bins=20, color='skyblue', edgecolor='black')\nplt.title(\"Distribution of Text Lengths\", fontsize=16)\nplt.xlabel(\"Number of Words\", fontsize=14)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.axvline(average_length, color='red', linestyle='dashed', linewidth=2, label=f'Average Length: {average_length:.2f}')\nplt.axvline(max_length, color='green', linestyle='dashed', linewidth=2, label=f'Max Length: {max_length}')\nplt.axvline(min_length, color='purple', linestyle='dashed', linewidth=2, label=f'Min Length: {min_length}')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_text = ' '.join(str(text) for text in df['label'])\n\nwords = all_text.split()\nword_counts = Counter(words)\n\nmost_common_words = word_counts.most_common(20)  # Adjust the number of words as needed\n\n# Create a bar chart\nwords, counts = zip(*most_common_words)\nplt.figure(figsize=(12, 6))\nplt.bar(words, counts)\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Most Common Words in Target Column')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_words = [word for text in df['text'] for word in str(text).split()]\nword_freq = Counter(all_words)\ntop_10_words = word_freq.most_common(20)\nwords, counts = zip(*top_10_words)\nplt.figure(figsize=(10, 6))\nplt.bar(words, counts, color='skyblue')\nplt.title(\"Top 20 Most Frequent Words\", fontsize=16)\nplt.xlabel(\"Words\", fontsize=14)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_text = ' '.join(df['text'].tolist())\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most Common Words in Text')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for label in df['label'].unique():\n    label_text = ' '.join(df[df['label'] == label]['text'].tolist())\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(label_text)\n    plt.figure(figsize=(10, 6))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    if label == 1:\n        plt.title('Most Common Words')\n    else:\n        plt.title('Most Common Words')\n    plt.show()\n    print()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_oversample(X, y):\n    # Combine the features and labels\n    df = pd.DataFrame({'text': X, 'label': y})\n    # Separate the majority and minority classes\n    df_majority = df[df.label == 1]\n    df_minority = df[df.label == 0]\n    \n    # Upsample minority class\n    df_minority_upsampled = resample(df_minority,\n                                      replace=True,     # sample with replacement\n                                      n_samples=len(df_majority),    # to match majority class\n                                      random_state=42) # reproducible results\n\n    # Combine majority class with upsampled minority class\n    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n    \n    return df_upsampled['text'].values, df_upsampled['label'].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.texts = dataframe['text'].values\n        self.labels = dataframe['label'].values\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=3, delta=0):\n        self.patience = patience\n        self.delta = delta\n        self.best_loss = None\n        self.counter = 0\n        self.early_stop = False\n    \n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss + self.delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader, test_dataloader, device, epochs=5, lr=1e-5, weight_decay=0.01):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    early_stopping = EarlyStopping(patience=3)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        total = 0\n        \n        for batch in tqdm(train_dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            train_correct += predicted.eq(labels.data).cpu().sum().float()\n\n        print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_dataloader):.4f}, Accuracy: {train_correct/total:.4f}')\n        \n        # Testing Step\n        model.eval()\n        test_loss = 0.0\n        test_correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in test_dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['label'].to(device)\n\n                outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs, labels)\n\n                test_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                test_correct += predicted.eq(labels.data).cpu().sum().float()\n\n        print(f'Test Loss: {test_loss/len(test_dataloader):.4f}, Test Accuracy: {test_correct/total:.4f}')\n\n        # Validation Step\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch in val_dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['label'].to(device)\n                \n                outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                val_correct += predicted.eq(labels.data).cpu().sum().float()\n\n        print(f'Validation Loss: {val_loss/len(val_dataloader):.4f}, Validation Accuracy: {val_correct/total:.4f}')\n        early_stopping(val_loss / len(val_dataloader))\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered!\")\n            break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_dataloader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            _, predicted = torch.max(outputs, 1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Generate and print the classification report\n    print(\"Classification Report on Test Dataset:\")\n    print(classification_report(all_labels, all_preds, target_names=['Class 0', 'Class 1']))\n#     print(classification_report(all_labels, all_preds, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3']))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\ntrain_df_X, train_df_y = random_oversample(train_df['text'].values, train_df['label'].values)\ntrain_df = pd.DataFrame({'text': train_df_X, 'label': train_df_y})\nprint(train_df['label'].value_counts())\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DistilBERT(nn.Module):\n    def __init__(self, n_classes):\n        super(DistilBERT, self).__init__()\n        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.fc = nn.Linear(768, n_classes)  # Classification head with n_classes output\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        cls_token_state = outputs.last_hidden_state[:, 0, :]  # CLS token for classification\n        output = self.fc(cls_token_state)\n        return output\n# Print the summary\nmodel_DistilBERT = DistilBERT(2).to(device)\nsummary(model_DistilBERT, input_size=[(1, 256), (1, 256)], dtypes=[torch.long, torch.long])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize tokenizer\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Prepare datasets\ntrain_distilbert_text_dataset = TextDataset(train_df, distilbert_tokenizer, max_len=256)\nval_distilbert_text_dataset = TextDataset(val_df, distilbert_tokenizer, max_len=256)\ntest_distilbert_text_dataset = TextDataset(test_df, distilbert_tokenizer, max_len=256)\n\n# Prepare dataloaders\ntrain_distilbert_text_dataloader = DataLoader(train_distilbert_text_dataset, batch_size=16, shuffle=True)\nval_distilbert_text_dataloader = DataLoader(val_distilbert_text_dataset, batch_size=16, shuffle=False)\ntest_distilbert_text_dataloader = DataLoader(test_distilbert_text_dataset, batch_size=16, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(\n    model_DistilBERT,  # Model instance\n    train_distilbert_text_dataloader,  # Training dataloader\n    val_distilbert_text_dataloader,    # Validation dataloader\n    test_distilbert_text_dataloader,   # Test dataloader\n    device, \n    epochs=100\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model_DistilBERT, test_distilbert_text_dataloader, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}